{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDd6lLwlx4un"
   },
   "source": [
    "# 1. **Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaPd82NmyNCK"
   },
   "outputs": [],
   "source": [
    "# Set image size\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define image data generator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Define the paths to the image files and the labels\n",
    "image_dir = 'C:/Users/maria/Documents/GitHub/CSCU9M6-2929300/MixDataset/train'\n",
    "imageA_dir = 'C:/Users/maria/Documents/GitHub/CSCU9M6-2929300/Dataset/Atrain'\n",
    "imageB_dir = 'C:/Users/maria/Documents/GitHub/CSCU9M6-2929300/Dataset/Btrain'\n",
    "label_file = 'C:/Users/maria/Documents/GitHub/CSCU9M6-2929300/'\n",
    "\n",
    "\n",
    "# Define the labels\n",
    "labels = ['ara', 'ban', 'che', 'pal', 'pla', 'art', 'ced', \n",
    "          'ash', 'syc', 'oak', 'app', 'pin', 'bee', 'bct', 'not']\n",
    "\n",
    "def load_and_resize(filename, label):\n",
    "    # Load the image\n",
    "    image = tf.io.read_file(filename)\n",
    "    # Decode the image\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [img_size[0], img_size[1]])\n",
    "    # Rescale the pixel values\n",
    "    image = image / 255.0\n",
    "    # Return the image and label\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(filenames, labels, batch_size):\n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=len(filenames), reshuffle_each_iteration=False)\n",
    "    # Load and preprocess the images and labels in parallel\n",
    "    dataset = dataset.map(\n",
    "        map_func=load_and_resize,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # Prefetch the dataset\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    # Return the dataset\n",
    "    return dataset\n",
    "\n",
    "# Create a function to load and preprocess the image and its label\n",
    "def load_and_preprocess_image(filename, label):\n",
    "    # Load the image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode the image\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    # Preprocess the image\n",
    "    image, label = preprocess_image(image_decoded, label)\n",
    "    # Return the image and its label\n",
    "    return image, label\n",
    "\n",
    "# Load the labels from the file\n",
    "with open(os.path.join(label_file, 'imageData.txt')) as f:\n",
    "    lines = f.readlines()\n",
    "    label_dict = {}\n",
    "    for line in lines:\n",
    "        filename, label, _ = line.strip().split('.', maxsplit=2) # added maxsplit to prevent an error i was\n",
    "        if label in labels:                                      # getting due to the way I named my files\n",
    "               label_dict[os.path.join(image_dir, filename)] = labels.index(label)\n",
    "        \n",
    "# Get the filenames and label\n",
    "filenames = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each folder in the directory\n",
    "for class_name in os.listdir(image_dir):\n",
    "    class_dir = os.path.join(image_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        filepath = os.path.join(class_dir, filename)\n",
    "        filenames.append(filepath)\n",
    "        labels.append(class_name)\n",
    "        \n",
    "# Create a train-validation split\n",
    "train_filenames, test_filenames, train_labels, test_labels = train_test_split(\n",
    "    filenames, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "Atrain_filenames, Atest_filenames, Atrain_labels, Atest_labels = train_test_split(\n",
    "    filenames, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "Btrain_filenames, Btest_filenames, Btrain_labels, Btest_labels = train_test_split(\n",
    "    filenames, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define data directories\n",
    "train_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/MixDataset/train'\n",
    "Atrain_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Atrain'\n",
    "Btrain_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Btrain'\n",
    "test_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/MixDataset/test'\n",
    "Atest_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Atest'\n",
    "Btest_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Btest'\n",
    "val_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/MixDataset/val'\n",
    "Aval_dir = \"C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Aval\"\n",
    "Bval_dir = 'C:/Users/maria/Documents/GitHub/Tree-Species-Classification-with-Deep-Learning/Dataset/Bval'\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "dataset = create_dataset(filenames, labels, batch_size)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScTrpUW8zOp4"
   },
   "source": [
    "---\n",
    "\n",
    "# 2. **Proposed solution** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJs8HpW_zX0M"
   },
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "\n",
    "# Load and preprocess the training and testing datasets\n",
    "train_dataset = create_dataset(train_filenames, train_labels, batch_size)\n",
    "test_dataset = create_dataset(test_filenames, test_labels, batch_size)\n",
    "\n",
    "# Define image data generator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(244, 244),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# create model using VGG16 architecture\n",
    "def create_model():\n",
    "    vgg16_model = tf.keras.applications.VGG16(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model = keras.models.Sequential([\n",
    "        keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(img_size[0]*2, img_size[1]*2, 3)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(15, activation='softmax')])\n",
    "    return model\n",
    "\n",
    "# Define the model\n",
    "input_shape = img_size + (3,)\n",
    "model = keras.models.Sequential([\n",
    "    keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(15, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RBW58of0ZDo"
   },
   "source": [
    "---\n",
    "\n",
    "# 3. **Experimental tests and evaluations** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHWwdXg32BEl"
   },
   "outputs": [],
   "source": [
    "# Training model for City A\n",
    "\n",
    "# Get the list of filenames in the directory\n",
    "Atrain_dir_filenames = os.listdir(Atrain_dir)\n",
    "Atest_dir_filenames = os.listdir(Atest_dir)\n",
    "Aval_dir_filenames = os.listdir(Aval_dir)\n",
    "\n",
    "# Load and preprocess the data using tf.data\n",
    "Atrain_data = create_dataset(Atrain_dir_filenames, Atrain_labels, batch_size)\n",
    "Aval_data = create_dataset(aval_dir_filenames, Aval_labels, batch_size)\n",
    "Atest_dataset = create_dataset(Atest_filenames, Atest_labels, batch_size)\n",
    "  \n",
    "# Build model\n",
    "model = tf.keras.models.Sequential([\n",
    "        vgg16_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(Atrain_images, train_labels, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('modelA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for City B\n",
    "\n",
    "# Get the list of filenames in the directory\n",
    "Btrain_dir_filenames = os.listdir(Btrain_dir)\n",
    "Btest_dir_filenames = os.listdir(Btest_dir)\n",
    "Bval_dir_filenames = os.listdir(Bval_dir)\n",
    "\n",
    "# Load and preprocess the data using tf.data\n",
    "Btrain_dataset = create_dataset(Btrain_filenames, Btrain_labels, batch_size)\n",
    "Bval_data = create_dataset(Bval_dir_filenames, Bval_labels, batch_size)\n",
    "Btest_dataset = create_dataset(Btest_filenames, Btest_labels, batch_size)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(Btrain_images, train_labels, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('modelB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s6ygCpi2CO8"
   },
   "outputs": [],
   "source": [
    "# Testing for City A\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(Atest_dataset)\n",
    "print('ATest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for City B\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(Btest_dataset, test_labels, verbose=2)\n",
    "print('BTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B for City A dataset\n",
    "\n",
    "# Load the saved model for City B\n",
    "model_B = load_model('modelB')\n",
    "\n",
    "# Load test data for City A\n",
    "Atest_filenames = os.listdir(Atest_dir)\n",
    "Atest_labels = [get_label_from_filename(\"label\") for filename in Atest_filenames]\n",
    "\n",
    "# Create test dataset for City A\n",
    "test_dataset = create_dataset(Atest_filenames, Atest_labels, batch_size)\n",
    "\n",
    "# Evaluate model on the test data for City A\n",
    "loss, accuracy = model_B.evaluate(Atest_dataset)\n",
    "\n",
    "# Print the test accuracy for City A\n",
    "print(\"Test accuracy for City A:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cotguzxyo3Sa"
   },
   "outputs": [],
   "source": [
    "# Model A for City B dataset\n",
    "\n",
    "# Load the saved model for City A\n",
    "model_A = tf.keras.models.load_model('modelA')\n",
    "\n",
    "# Load the test data for City B\n",
    "Btest_filenames = os.listdir(Btest_dir)\n",
    "Btest_labels = [get_label_from_filename(\"label\") for filename in Btest_filenames]\n",
    "\n",
    "# Create the test dataset for City B\n",
    "Btest_dataset = create_dataset(Btest_filenames, Btest_labels, batch_size)\n",
    "\n",
    "# Evaluate the model on the test data for City B\n",
    "loss, accuracy = model_A.evaluate(Btest_dataset)\n",
    "\n",
    "# Print the test accuracy for City B\n",
    "print(\"Test accuracy for City B:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
